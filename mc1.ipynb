{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.01\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Select Task/Dataset\n",
    "I chose Tiny ImageNet, which contains 100000 images of 200 classes (500 for each class) resized to 64x64 color images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from classes import i2d\n",
    "# import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset('Maysee/tiny-imagenet', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"dataset_infos.json\") as file:\n",
    "#     dataset_infos = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names = dataset_infos[\"Maysee--tiny-imagenet\"][\"features\"][\"label\"][\"names\"]\n",
    "# idx2class = {i: class_names[i] for i in range(len(class_names))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Get to know the data\n",
    "The dataset is well balanced and has 500 images for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# class_counts = defaultdict(int)\n",
    "# for instance in dataset:\n",
    "#     label = instance['label']\n",
    "#     class_counts[label] += 1\n",
    "\n",
    "# for label, count in class_counts.items():\n",
    "#     print(f\"Class {label}: {count} instances\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Structure Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1 Determine how (with which metrics) you want to evaluate your model. Also, consider the error in estimating the metrics.\n",
    "We will use accuracy and F1 macro (precision, recall) to evaluate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2 Implement basic functionality to train models and evaluate them against each other. It is recommended to use a suitable MLOps platform (e.g. W&B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haris.alic/Dev/github/hariveliki/del/tin.py:8: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from tin import TinyImageNetDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b882259d1db447596d0c9a92b5c1dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preloading train data...:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458d5aad6ed04904aaf8e94690a60330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preloading val data...:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca44320277e4526821889324e7e98ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preloading test data...:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a custom Dataset class because the dataset from load_dataset() is useless\n",
    "train_data = TinyImageNetDataset(root_dir=\"./data/tiny-imagenet-200\", mode=\"train\")\n",
    "val_data = TinyImageNetDataset(root_dir=\"./data/tiny-imagenet-200\", mode=\"val\")\n",
    "test_data = TinyImageNetDataset(root_dir=\"./data/tiny-imagenet-200\", mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data size: 100000\n",
      "val_data size: 10000\n",
      "test_data size: 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# reduce the size of train_data by x\n",
    "train_data = torch.utils.data.Subset(train_data, range(0, len(train_data), 1))\n",
    "print(f\"train_data size: {len(train_data)}\")\n",
    "val_data = torch.utils.data.Subset(val_data, range(0, len(val_data), 1))\n",
    "print(f\"val_data size: {len(val_data)}\")\n",
    "test_data = torch.utils.data.Subset(test_data, range(0, len(test_data), 1))\n",
    "print(f\"test_data size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1563 batches in the training set\n",
      "There are 157 batches in the validation set\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(f\"There are {len(train_loader)} batches in the training set\")\n",
    "print(f\"There are {len(val_loader)} batches in the validation set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2.1: Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import utils\n",
    "from typing import List\n",
    "\n",
    "class CNN_MLP(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            num_classes: int,\n",
    "            layers: list,\n",
    "            confs: List[dict],\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            weight_init=None):\n",
    "        super(CNN_MLP, self).__init__()\n",
    "\n",
    "        self.net = nn.ModuleList()\n",
    "\n",
    "        for layer, conf in zip(layers, confs):\n",
    "            if layer == \"C\":\n",
    "                self.net.append(\n",
    "                    nn.Conv2d(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=conf[\"kernel\"],\n",
    "                        stride=conf[\"stride\"],\n",
    "                        padding=conf[\"padding\"]\n",
    "                    )\n",
    "                )\n",
    "                self.net.append(nn.ReLU())\n",
    "                in_channels = out_channels\n",
    "                out_channels *= 2\n",
    "            elif layer == \"P\":\n",
    "                self.net.append(\n",
    "                    nn.MaxPool2d(kernel_size=conf[\"kernel\"])\n",
    "                )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.in_channels = in_channels\n",
    "        self.dim = utils.get_dim_after_conv_and_pool(\n",
    "            dim_init=dim,\n",
    "            layers=layers,\n",
    "            confs=confs\n",
    "        )\n",
    "        print(f\"self.dim: {self.dim},\\nself.in_channels: {self.in_channels}\")\n",
    "        self.fc1 = nn.Linear(self.dim * self.dim * self.in_channels, 784)\n",
    "        self.fc2 = nn.Linear(784, 196)\n",
    "        self.fc3 = nn.Linear(196, num_classes)\n",
    "        # Weight Initialization\n",
    "        # self.initialize_weights(weight_init)\n",
    "        # if weight_init == \"random_uniform\":\n",
    "        #     assert (self.conv1.weight >= 0).all() and (self.conv1.weight <= 1).all()\n",
    "        #     assert (self.fc1.weight >= 0).all() and (self.fc1.weight <= 1).all()\n",
    "        # elif weight_init == \"random_normal\":\n",
    "        #     assert torch.isclose(self.conv1.weight.mean(), torch.tensor(0.), atol=1e-2).item()\n",
    "        #     assert torch.isclose(self.conv1.weight.std(), torch.tensor(1.), atol=1e-2).item()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        N = x.shape[0]\n",
    "        C, H, W = x.shape[3], x.shape[1], x.shape[2]\n",
    "        x = x.permute(0, 3, 1, 2)  # From (batch_size, H, W, C) to (batch_size, C, H, W)\n",
    "        assert x.shape == (N, C, H, W)\n",
    "        \n",
    "        for layer in self.net:\n",
    "            x = layer(x)\n",
    "        x = self.flatten(x)\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = nn.ReLU()(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    # def initialize_weights(self, kind):\n",
    "    #     if kind == \"random_uniform\":\n",
    "    #         for m in self.modules():\n",
    "    #             if isinstance(m, nn.Conv2d):\n",
    "    #                 nn.init.uniform_(m.weight)\n",
    "    #                 if m.bias is not None:\n",
    "    #                     nn.init.uniform_(m.bias)\n",
    "    #             elif isinstance(m, nn.Linear):\n",
    "    #                 nn.init.uniform_(m.weight)\n",
    "    #                 nn.init.uniform_(m.bias)\n",
    "    #     elif kind == \"random_normal\":\n",
    "    #         for m in self.modules():\n",
    "    #             if isinstance(m, nn.Conv2d):\n",
    "    #                 nn.init.normal_(m.weight)\n",
    "    #                 if m.bias is not None:\n",
    "    #                     nn.init.normal_(m.bias)\n",
    "    #             elif isinstance(m, nn.Linear):\n",
    "    #                 nn.init.normal_(m.weight)\n",
    "    #                 nn.init.normal_(m.bias)\n",
    "    #     elif kind == \"xavier\":\n",
    "    #         pass\n",
    "    #     elif kind == \"he\":\n",
    "    #         pass\n",
    "    #     else:\n",
    "    #         raise ValueError(\"Invalid weight initialization kind!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2.2: Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.dim: 14,\n",
      "self.in_channels: 32\n",
      "Epochs: 20\n",
      "Batch size: 64\n",
      "Learning rate: 0.01\n",
      "Seed: 42\n",
      "\n",
      "CNN_MLP(\n",
      "  (net): ModuleList(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=6272, out_features=784, bias=True)\n",
      "  (fc2): Linear(in_features=784, out_features=196, bias=True)\n",
      "  (fc3): Linear(in_features=196, out_features=200, bias=True)\n",
      ")\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─ModuleList: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       448\n",
      "|    └─ReLU: 2-2                         --\n",
      "|    └─MaxPool2d: 2-3                    --\n",
      "|    └─Conv2d: 2-4                       4,640\n",
      "|    └─ReLU: 2-5                         --\n",
      "|    └─MaxPool2d: 2-6                    --\n",
      "├─Flatten: 1-2                           --\n",
      "├─Linear: 1-3                            4,918,032\n",
      "├─Linear: 1-4                            153,860\n",
      "├─Linear: 1-5                            39,400\n",
      "=================================================================\n",
      "Total params: 5,116,380\n",
      "Trainable params: 5,116,380\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─ModuleList: 1-1                        --\n",
       "|    └─Conv2d: 2-1                       448\n",
       "|    └─ReLU: 2-2                         --\n",
       "|    └─MaxPool2d: 2-3                    --\n",
       "|    └─Conv2d: 2-4                       4,640\n",
       "|    └─ReLU: 2-5                         --\n",
       "|    └─MaxPool2d: 2-6                    --\n",
       "├─Flatten: 1-2                           --\n",
       "├─Linear: 1-3                            4,918,032\n",
       "├─Linear: 1-4                            153,860\n",
       "├─Linear: 1-5                            39,400\n",
       "=================================================================\n",
       "Total params: 5,116,380\n",
       "Trainable params: 5,116,380\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "layers = [\"C\", \"P\", \"C\", \"P\"]\n",
    "confs = [\n",
    "    {\"kernel\": 3, \"stride\": 1, \"padding\": 0},\n",
    "    {\"kernel\": 2},\n",
    "    {\"kernel\": 3, \"stride\": 1, \"padding\": 0},\n",
    "    {\"kernel\": 2}\n",
    "]\n",
    "model = CNN_MLP(\n",
    "    dim=64,\n",
    "    num_classes=200,\n",
    "    layers=layers,\n",
    "    confs=confs,\n",
    "    in_channels=3,\n",
    "    out_channels=16\n",
    ")\n",
    "# x = torch.randn(5, 64, 64, 3)\n",
    "# output = model(x)\n",
    "# print(output.shape)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(f\"Epochs: {EPOCHS}\\nBatch size: {BATCH_SIZE}\\nLearning rate: {LEARNING_RATE}\\nSeed: {SEED}\\n\")\n",
    "print(model)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2.3: Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=true\n",
      "env: PYTORCH_ENABLE_MPS_FALLBACK=1\n",
      "|---------------------------| Start Epoch 1: |---------------------------|\n",
      "Train Loss: 4.912283492561189\n",
      "Train Accuracy: 4.325\n",
      "Eval Loss: 4.558134874720483\n",
      "Eval Accuracy: 7.35\n",
      "|---------------------------| Start Epoch 2: |---------------------------|\n",
      "Train Loss: 4.297701539096356\n",
      "Train Accuracy: 10.89\n",
      "Eval Loss: 4.37727343200878\n",
      "Eval Accuracy: 9.3\n",
      "|---------------------------| Start Epoch 3: |---------------------------|\n",
      "Train Loss: 3.857043754421437\n",
      "Train Accuracy: 17.445\n",
      "Eval Loss: 4.20678383225848\n",
      "Eval Accuracy: 13.13\n",
      "|---------------------------| Start Epoch 4: |---------------------------|\n",
      "Train Loss: 3.424139152790443\n",
      "Train Accuracy: 24.037\n",
      "Eval Loss: 4.309830746073632\n",
      "Eval Accuracy: 13.23\n",
      "|---------------------------| Start Epoch 5: |---------------------------|\n",
      "Train Loss: 2.9049077271957544\n",
      "Train Accuracy: 32.836\n",
      "Eval Loss: 4.319305293878932\n",
      "Eval Accuracy: 15.45\n",
      "|---------------------------| Start Epoch 6: |---------------------------|\n",
      "Train Loss: 2.270758689098151\n",
      "Train Accuracy: 44.685\n",
      "Eval Loss: 4.954347297644159\n",
      "Eval Accuracy: 13.39\n",
      "|---------------------------| Start Epoch 7: |---------------------------|\n",
      "Train Loss: 1.6160389096479117\n",
      "Train Accuracy: 58.441\n",
      "Eval Loss: 5.697088519479059\n",
      "Eval Accuracy: 12.49\n",
      "|---------------------------| Start Epoch 8: |---------------------------|\n",
      "Train Loss: 1.111336470813379\n",
      "Train Accuracy: 70.19\n",
      "Eval Loss: 7.023866844784682\n",
      "Eval Accuracy: 10.59\n",
      "|---------------------------| Start Epoch 9: |---------------------------|\n",
      "Train Loss: 0.8106236707321437\n",
      "Train Accuracy: 77.577\n",
      "Eval Loss: 7.460685283515104\n",
      "Eval Accuracy: 10.78\n",
      "|---------------------------| Start Epoch 10: |---------------------------|\n",
      "Train Loss: 0.6452876933000061\n",
      "Train Accuracy: 81.788\n",
      "Eval Loss: 8.31004425826346\n",
      "Eval Accuracy: 11.3\n",
      "|---------------------------| Start Epoch 11: |---------------------------|\n",
      "Train Loss: 0.5251657944971068\n",
      "Train Accuracy: 85.15\n",
      "Eval Loss: 9.30970212608386\n",
      "Eval Accuracy: 11.1\n",
      "|---------------------------| Start Epoch 12: |---------------------------|\n",
      "Train Loss: 0.4509742580358981\n",
      "Train Accuracy: 87.221\n",
      "Eval Loss: 9.577105546453197\n",
      "Eval Accuracy: 10.4\n",
      "|---------------------------| Start Epoch 13: |---------------------------|\n",
      "Train Loss: 0.42462024367482937\n",
      "Train Accuracy: 88.096\n",
      "Eval Loss: 9.662628793412713\n",
      "Eval Accuracy: 10.28\n",
      "|---------------------------| Start Epoch 14: |---------------------------|\n",
      "Train Loss: 0.3606885697380404\n",
      "Train Accuracy: 89.724\n",
      "Eval Loss: 10.5590452145619\n",
      "Eval Accuracy: 11.18\n",
      "|---------------------------| Start Epoch 15: |---------------------------|\n",
      "Train Loss: 0.32761690506302843\n",
      "Train Accuracy: 90.794\n",
      "Eval Loss: 11.387794105869949\n",
      "Eval Accuracy: 10.91\n",
      "|---------------------------| Start Epoch 16: |---------------------------|\n",
      "Train Loss: 0.3210201891328632\n",
      "Train Accuracy: 91.168\n",
      "Eval Loss: 11.51265161356349\n",
      "Eval Accuracy: 10.78\n",
      "|---------------------------| Start Epoch 17: |---------------------------|\n",
      "Train Loss: 0.3161636185275197\n",
      "Train Accuracy: 91.333\n",
      "Eval Loss: 11.912090386554693\n",
      "Eval Accuracy: 10.5\n",
      "|---------------------------| Start Epoch 18: |---------------------------|\n",
      "Train Loss: 0.29259941699394454\n",
      "Train Accuracy: 92.031\n",
      "Eval Loss: 12.315641081257231\n",
      "Eval Accuracy: 9.7\n",
      "|---------------------------| Start Epoch 19: |---------------------------|\n",
      "Train Loss: 0.2995097829649846\n",
      "Train Accuracy: 92.114\n",
      "Eval Loss: 12.834505050804964\n",
      "Eval Accuracy: 10.08\n",
      "|---------------------------| Start Epoch 20: |---------------------------|\n",
      "Train Loss: 0.29944509614029\n",
      "Train Accuracy: 92.021\n",
      "Eval Loss: 13.607485546427927\n",
      "Eval Accuracy: 10.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "# %env WANDB_LOG_MODEL=\"end\"\n",
    "%env WANDB_SILENT=true\n",
    "%env PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "try:\n",
    "    wandb.login()\n",
    "    wandb.init(project=\"del\",\n",
    "               entity=\"hariveliki\")\n",
    "    wandb.define_metric(\"epoch\")\n",
    "    wandb.define_metric(\"loss_train\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"loss_eval\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"train_accuracy\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"eval_accuracy\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"f1_macro\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"precision\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"recall\", step_metric=\"epoch\")\n",
    "\n",
    "    # conv1_grads = []\n",
    "    # def save_grad(module, grad_input, grad_output):\n",
    "    #     conv1_grads.append(grad_output[0])\n",
    "    # hook_handle = model.conv1.register_full_backward_hook(save_grad)\n",
    "\n",
    "    model.to(device)\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print(f\"|---------------------------| Start Epoch {epoch}: |---------------------------|\")\n",
    "        loss_train = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        model.train()\n",
    "        for n, batch in enumerate(train_loader):\n",
    "            imgs = batch[\"image\"]\n",
    "            imgs = imgs.to(device)\n",
    "            labels = batch[\"label\"]\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            labels = labels.long()\n",
    "            logits = model(imgs)\n",
    "            preds = nn.functional.softmax(logits, dim=1)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss_train += loss.item()\n",
    "            predicted = preds.argmax(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        loss_train /= len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        print(f\"Train Loss: {loss_train}\")\n",
    "        print(f\"Train Accuracy: {train_accuracy}\")\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_eval = 0\n",
    "        # all_labels = []\n",
    "        # all_outputs = []\n",
    "        model.eval()\n",
    "        for n, batch in enumerate(val_loader):\n",
    "            imgs = batch[\"image\"]\n",
    "            imgs = imgs.to(device)\n",
    "            labels = batch[\"label\"]\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            labels = labels.long()\n",
    "            logits = model(imgs)\n",
    "            preds = nn.functional.softmax(logits, dim=1)\n",
    "            loss_eval += criterion(logits, labels).item()\n",
    "            predicted = preds.argmax(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # correct += (preds.argmax(1) == labels).sum().item()\n",
    "            # all_labels.extend(preds.argmax(1).cpu().detach().numpy().tolist())\n",
    "            # all_outputs.extend(labels.cpu().detach().numpy().tolist())\n",
    "\n",
    "        loss_eval /= len(val_loader)\n",
    "        # correct /= len(val_loader)\n",
    "        eval_accuracy = 100 * correct / total\n",
    "        # f1_macro = f1_score(all_labels, all_outputs, average='macro')\n",
    "        # precision = precision_score(all_labels, all_outputs, average='macro', zero_division=0)\n",
    "        # recall = recall_score(all_labels, all_outputs, average='macro', zero_division=0)\n",
    "        print(f\"Eval Loss: {loss_eval}\")\n",
    "        print(f\"Eval Accuracy: {eval_accuracy}\")\n",
    "        # print(f\"F1 Macro: {f1_macro}\")\n",
    "        # print(f\"Precision: {precision}\")\n",
    "        # print(f\"Recall: {recall}\")\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"loss_train\": loss_train,\n",
    "                \"loss_eval\": loss_eval,\n",
    "                \"train_accuracy\": train_accuracy,\n",
    "                \"eval_accuracy\": eval_accuracy\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # hook_handle.remove()\n",
    "    wandb.finish()\n",
    "\n",
    "except Exception as e:\n",
    "    wandb.finish()\n",
    "    raise e\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    wandb.finish()\n",
    "    raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_conv_layer_gradients(batch_idx, conv_grads):\n",
    "    \"\"\"\n",
    "    Plots the gradients of a convolutional layer for a given batch index.\n",
    "\n",
    "    Args:\n",
    "    - batch_idx (int): The index of the batch to plot the gradients for.\n",
    "    - conv_grads (list): A list of the gradients of the convolutional layer.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Get the gradient for that batch\n",
    "    batch_grad = conv_grads[batch_idx]\n",
    "\n",
    "    # Convert the PyTorch tensor to a NumPy array\n",
    "    batch_grad_np = batch_grad.cpu().numpy()\n",
    "\n",
    "    # Create a plot\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # Assuming the weight tensor is of shape (out_channels, in_channels, kernel_size, kernel_size)\n",
    "    out_channels = batch_grad_np.shape[0]\n",
    "    for i in range(out_channels):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(batch_grad_np[i, 0], cmap='viridis')\n",
    "        plt.title(f'Output channel {i}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# plot_conv_layer_gradients(0, conv1_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "del",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
