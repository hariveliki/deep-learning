{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Select Task/Dataset\n",
    "I chose Tiny ImageNet, which contains 100000 images of 200 classes (500 for each class) resized to 64x64 color images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from classes import i2d\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/haris.alic/.cache/huggingface/datasets/Maysee___parquet/Maysee--tiny-imagenet-2eb6c3acd8ebc62a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('Maysee/tiny-imagenet', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64>,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset_infos.json\") as file:\n",
    "    dataset_infos = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = dataset_infos[\"Maysee--tiny-imagenet\"][\"features\"][\"label\"][\"names\"]\n",
    "idx2class = {i: class_names[i] for i in range(len(class_names))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Get to know the data\n",
    "The dataset is well balanced and has 500 images for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# class_counts = defaultdict(int)\n",
    "# for instance in dataset:\n",
    "#     label = instance['label']\n",
    "#     class_counts[label] += 1\n",
    "\n",
    "# for label, count in class_counts.items():\n",
    "#     print(f\"Class {label}: {count} instances\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Structure Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1 Determine how (with which metrics) you want to evaluate your model. Also, consider the error in estimating the metrics.\n",
    "We will use accuracy, precision, recall, and F1 score (macro) to evaluate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2 Implement basic functionality to train models and evaluate them against each other. It is recommended to use a suitable MLOps platform (e.g. W&B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_LOG_MODEL=\"end\"\n",
      "env: WANDB_SILENT=true\n",
      "env: PYTORCH_ENABLE_MPS_FALLBACK=1\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_LOG_MODEL=\"end\"\n",
    "%env WANDB_SILENT=true\n",
    "%env PYTORCH_ENABLE_MPS_FALLBACK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = \"mc1-cnn-mlp\"\n",
    "# wandb_group = checkpoint.split(\"-\")[0]\n",
    "# wandb_name = \"-\".join(checkpoint.split(\"-\")[1:])\n",
    "# print(f\"wandb_group: {wandb_group}\")\n",
    "# print(f\"wandb_name: {wandb_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tin import TinyImageNetDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26037260134417283c7e0c0668c9e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preloading train data...:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f79293ff8e540068545e3ae62f94248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preloading val data...:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be85e7e580904aa795136128f8273a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preloading test data...:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a custom Dataset class because the dataset from load_dataset() is useless\n",
    "train_data = TinyImageNetDataset(root_dir=\"./data/tiny-imagenet-200\", mode=\"train\")\n",
    "val_data = TinyImageNetDataset(root_dir=\"./data/tiny-imagenet-200\", mode=\"val\")\n",
    "test_data = TinyImageNetDataset(root_dir=\"./data/tiny-imagenet-200\", mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data size: 200\n",
      "val_data size: 100\n",
      "test_data size: 100\n"
     ]
    }
   ],
   "source": [
    "# reduce the size of train_data by x\n",
    "train_data = torch.utils.data.Subset(train_data, range(0, len(train_data), 500))\n",
    "print(f\"train_data size: {len(train_data)}\")\n",
    "val_data = torch.utils.data.Subset(val_data, range(0, len(val_data), 100))\n",
    "print(f\"val_data size: {len(val_data)}\")\n",
    "test_data = torch.utils.data.Subset(test_data, range(0, len(test_data), 100))\n",
    "print(f\"test_data size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 200 batches in the training set\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "print(f\"There are {len(trainloader)} batches in the training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_MLP(nn.Module):\n",
    "    def __init__(self, num_classes=200):\n",
    "        super(CNN_MLP, self).__init__()\n",
    "        # Convolutional Layer that takes an input tensor with 3 channels\n",
    "        # and outputs a tensor with 16 channels\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,\n",
    "                               out_channels=16,\n",
    "                               kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16,\n",
    "                               out_channels=32,\n",
    "                               kernel_size=3)\n",
    "        # Flattens the input tensor into a 1D tensor\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.permute(0, 3, 1, 2)  # From (batch_size, H, W, C) to (batch_size, C, H, W)\n",
    "        assert x.shape == (batch_size, 3, 64, 64)\n",
    "        \n",
    "        # # TODO understand -> using ReLU (non-saturating activation functions) to alleviate the vanishing gradients problem\n",
    "        x = self.conv1(x)\n",
    "        assert x.shape == (batch_size, 16, 62, 62)\n",
    "        x = nn.ReLU()(x)\n",
    "        # Applies max-pooling to reduce the spatial dimensions of the tensor\n",
    "        x = nn.MaxPool2d(kernel_size=2)(x)\n",
    "        assert x.shape == (batch_size, 16, 31, 31)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        assert x.shape == (batch_size, 32, 29, 29)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = nn.MaxPool2d(kernel_size=2)(x)\n",
    "        assert x.shape == (batch_size, 32, 14, 14)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = nn.ReLU()(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_MLP(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=6272, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=200, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CNN_MLP()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(model)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.00040534863364882767\n",
      "loss: 3.0874729418428615e-05\n",
      "loss: 0.049919359385967255\n",
      "loss: 1.6760648488998413\n",
      "loss: 1.105322241783142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "wandb.login()\n",
    "wandb.init(project=\"del\",\n",
    "           config={\"epochs\": EPOCHS})\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for n, batch in enumerate(trainloader):\n",
    "        imgs = batch[\"image\"]\n",
    "        assert imgs.shape == (batch_size, 64, 64, 3)\n",
    "        labels = batch[\"label\"]\n",
    "        assert labels.shape == (batch_size,)\n",
    "\n",
    "        # Forward pass\n",
    "        labels = labels.long()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"loss: {loss}\")\n",
    "    wandb.log({\"loss\": loss})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
