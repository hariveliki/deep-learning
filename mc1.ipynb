{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Select Task/Dataset\n",
    "I chose Tiny ImageNet, which contains 100000 images of 200 classes (500 for each class) resized to 64x64 color images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from classes import i2d\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/haris.alic/.cache/huggingface/datasets/Maysee___parquet/Maysee--tiny-imagenet-2eb6c3acd8ebc62a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('Maysee/tiny-imagenet', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64>,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset_infos.json\") as file:\n",
    "    dataset_infos = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = dataset_infos[\"Maysee--tiny-imagenet\"][\"features\"][\"label\"][\"names\"]\n",
    "idx2class = {i: class_names[i] for i in range(len(class_names))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Get to know the data\n",
    "The dataset is well balanced and has 500 images for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class_counts = defaultdict(int)\n",
    "for instance in dataset:\n",
    "    label = instance['label']\n",
    "    class_counts[label] += 1\n",
    "\n",
    "plt.bar(class_counts.keys(), class_counts.values())\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title(\"Occurences per class\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Structure Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1 Determine how (with which metrics) you want to evaluate your model. Also, consider the error in estimating the metrics.\n",
    "We will use accuracy to evaluate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2 Implement basic functionality to train models and evaluate them against each other. It is recommended to use a suitable MLOps platform (e.g. W&B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haris.alic/Dev/github/hariveliki/del/tin.py:8: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from tin import TinyImageNetDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17cef983cf46479b9abc50a2413da6d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preloading train data...:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d5222888d24d68aabf0f85ce7e5d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preloading val data...:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4b7426b89142f0a57cae7ff570ae55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preloading test data...:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a custom Dataset class because the dataset from load_dataset() is useless\n",
    "train_data = TinyImageNetDataset(root_dir=\"./data/tiny-imagenet-200\", mode=\"train\")\n",
    "val_data = TinyImageNetDataset(root_dir=\"./data/tiny-imagenet-200\", mode=\"val\")\n",
    "test_data = TinyImageNetDataset(root_dir=\"./data/tiny-imagenet-200\", mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data size: 100000\n",
      "val_data size: 10000\n",
      "test_data size: 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# reduce the size of train_data by x\n",
    "train_data = torch.utils.data.Subset(train_data, range(0, len(train_data), 1))\n",
    "print(f\"train_data size: {len(train_data)}\")\n",
    "val_data = torch.utils.data.Subset(val_data, range(0, len(val_data), 1))\n",
    "print(f\"val_data size: {len(val_data)}\")\n",
    "test_data = torch.utils.data.Subset(test_data, range(0, len(test_data), 1))\n",
    "print(f\"test_data size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1563 batches in the training set\n",
      "There are 157 batches in the validation set\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(f\"There are {len(train_loader)} batches in the training set\")\n",
    "print(f\"There are {len(val_loader)} batches in the validation set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2.1: Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import utils\n",
    "from typing import List\n",
    "\n",
    "class CNN_MLP(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            num_classes: int,\n",
    "            layers: list,\n",
    "            confs: List[dict],\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            dropout=0.5,\n",
    "            weight_init=None):\n",
    "        super(CNN_MLP, self).__init__()\n",
    "\n",
    "        self.net = nn.ModuleList()\n",
    "\n",
    "        for layer, conf in zip(layers, confs):\n",
    "            if layer == \"C\":\n",
    "                self.net.append(\n",
    "                    nn.Conv2d(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=conf[\"kernel\"],\n",
    "                        stride=conf[\"stride\"],\n",
    "                        padding=conf[\"padding\"]\n",
    "                    )\n",
    "                )\n",
    "                # self.net.append(nn.BatchNorm2d(out_channels))\n",
    "                self.net.append(nn.ReLU())\n",
    "                self.net.append(nn.BatchNorm2d(out_channels))\n",
    "                in_channels = out_channels\n",
    "                out_channels = conf[\"channels\"]\n",
    "            elif layer == \"P\":\n",
    "                self.net.append(\n",
    "                    nn.MaxPool2d(kernel_size=conf[\"kernel\"])\n",
    "                )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.in_channels = in_channels\n",
    "        self.dim = utils.get_dim_after_conv_and_pool(\n",
    "            dim_init=dim,\n",
    "            layers=layers,\n",
    "            confs=confs\n",
    "        )\n",
    "        print(f\"self.dim: {self.dim},\\nself.in_channels: {self.in_channels}\")\n",
    "        self.fc1 = nn.Linear(self.dim * self.dim * self.in_channels, 500)\n",
    "        # self.fc2 = nn.Linear(500, 500)\n",
    "        self.fc3 = nn.Linear(500, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        N = x.shape[0]\n",
    "        C, H, W = x.shape[3], x.shape[1], x.shape[2]\n",
    "        x = x.permute(0, 3, 1, 2)  # From (batch_size, H, W, C) to (batch_size, C, H, W)\n",
    "        assert x.shape == (N, C, H, W)\n",
    "        \n",
    "        for layer in self.net:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "BATCH_SIZE = BATCH_SIZE\n",
    "LEARNING_RATE = 0.001\n",
    "SEED = 42\n",
    "WEIGHT_DECAY = None\n",
    "DROPOUT = 0.5\n",
    "BATCH_NORM = True\n",
    "OPTIMIZER = \"Adam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2.2: Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.dim: 8,\n",
      "self.in_channels: 128\n",
      "Epochs: 40\n",
      "Batch size: 64\n",
      "Learning rate: 0.001\n",
      "Seed: 42\n",
      "Weight Decay: None\n",
      "Dropout: 0.5\n",
      "Batch Norm: True\n",
      "Optimizer: Adam\n",
      "CNN_MLP(\n",
      "  (net): ModuleList(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU()\n",
      "    (9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU()\n",
      "    (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU()\n",
      "    (19): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=8192, out_features=500, bias=True)\n",
      "  (fc3): Linear(in_features=500, out_features=200, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─ModuleList: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       448\n",
      "|    └─ReLU: 2-2                         --\n",
      "|    └─BatchNorm2d: 2-3                  32\n",
      "|    └─Conv2d: 2-4                       2,320\n",
      "|    └─ReLU: 2-5                         --\n",
      "|    └─BatchNorm2d: 2-6                  32\n",
      "|    └─MaxPool2d: 2-7                    --\n",
      "|    └─Conv2d: 2-8                       4,640\n",
      "|    └─ReLU: 2-9                         --\n",
      "|    └─BatchNorm2d: 2-10                 64\n",
      "|    └─Conv2d: 2-11                      18,496\n",
      "|    └─ReLU: 2-12                        --\n",
      "|    └─BatchNorm2d: 2-13                 128\n",
      "|    └─MaxPool2d: 2-14                   --\n",
      "|    └─Conv2d: 2-15                      36,928\n",
      "|    └─ReLU: 2-16                        --\n",
      "|    └─BatchNorm2d: 2-17                 128\n",
      "|    └─Conv2d: 2-18                      73,856\n",
      "|    └─ReLU: 2-19                        --\n",
      "|    └─BatchNorm2d: 2-20                 256\n",
      "|    └─MaxPool2d: 2-21                   --\n",
      "├─Flatten: 1-2                           --\n",
      "├─Linear: 1-3                            4,096,500\n",
      "├─Linear: 1-4                            100,200\n",
      "├─Dropout: 1-5                           --\n",
      "=================================================================\n",
      "Total params: 4,334,028\n",
      "Trainable params: 4,334,028\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─ModuleList: 1-1                        --\n",
       "|    └─Conv2d: 2-1                       448\n",
       "|    └─ReLU: 2-2                         --\n",
       "|    └─BatchNorm2d: 2-3                  32\n",
       "|    └─Conv2d: 2-4                       2,320\n",
       "|    └─ReLU: 2-5                         --\n",
       "|    └─BatchNorm2d: 2-6                  32\n",
       "|    └─MaxPool2d: 2-7                    --\n",
       "|    └─Conv2d: 2-8                       4,640\n",
       "|    └─ReLU: 2-9                         --\n",
       "|    └─BatchNorm2d: 2-10                 64\n",
       "|    └─Conv2d: 2-11                      18,496\n",
       "|    └─ReLU: 2-12                        --\n",
       "|    └─BatchNorm2d: 2-13                 128\n",
       "|    └─MaxPool2d: 2-14                   --\n",
       "|    └─Conv2d: 2-15                      36,928\n",
       "|    └─ReLU: 2-16                        --\n",
       "|    └─BatchNorm2d: 2-17                 128\n",
       "|    └─Conv2d: 2-18                      73,856\n",
       "|    └─ReLU: 2-19                        --\n",
       "|    └─BatchNorm2d: 2-20                 256\n",
       "|    └─MaxPool2d: 2-21                   --\n",
       "├─Flatten: 1-2                           --\n",
       "├─Linear: 1-3                            4,096,500\n",
       "├─Linear: 1-4                            100,200\n",
       "├─Dropout: 1-5                           --\n",
       "=================================================================\n",
       "Total params: 4,334,028\n",
       "Trainable params: 4,334,028\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "layers = [\"C\", \"C\", \"P\", \"C\", \"C\", \"P\", \"C\", \"C\", \"P\"]\n",
    "confs = [\n",
    "    {\"kernel\": 3, \"stride\": 1, \"padding\": 1, \"channels\": 16},\n",
    "    {\"kernel\": 3, \"stride\": 1, \"padding\": 1, \"channels\": 32},\n",
    "    {\"kernel\": 2},\n",
    "    {\"kernel\": 3, \"stride\": 1, \"padding\": 1, \"channels\": 64},\n",
    "    {\"kernel\": 3, \"stride\": 1, \"padding\": 1, \"channels\": 64},\n",
    "    {\"kernel\": 2},\n",
    "    {\"kernel\": 3, \"stride\": 1, \"padding\": 1, \"channels\": 128},\n",
    "    {\"kernel\": 3, \"stride\": 1, \"padding\": 1, \"channels\": 128},\n",
    "    {\"kernel\": 2},\n",
    "]\n",
    "\n",
    "model = CNN_MLP(\n",
    "    dim=64,\n",
    "    num_classes=200,\n",
    "    layers=layers,\n",
    "    confs=confs,\n",
    "    in_channels=3,\n",
    "    out_channels=16,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(f\"Epochs: {EPOCHS}\\nBatch size: {BATCH_SIZE}\\n\\\n",
    "Learning rate: {LEARNING_RATE}\\nSeed: {SEED}\\nWeight Decay: {WEIGHT_DECAY}\\n\\\n",
    "Dropout: {DROPOUT}\\nBatch Norm: {BATCH_NORM}\\nOptimizer: {OPTIMIZER}\")\n",
    "print(model)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2.3: Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=true\n",
      "env: PYTORCH_ENABLE_MPS_FALLBACK=1\n",
      "|---------------------------| Start Epoch 1: |---------------------------|\n",
      "Train Loss: 5.221913453637219\n",
      "Train Accuracy: 1.096\n",
      "Eval Loss: 5.042434437259747\n",
      "Eval Accuracy: 1.9\n",
      "|---------------------------| Start Epoch 2: |---------------------------|\n",
      "Train Loss: 5.102122197605751\n",
      "Train Accuracy: 1.419\n",
      "Eval Loss: 4.910566876648338\n",
      "Eval Accuracy: 2.88\n",
      "|---------------------------| Start Epoch 3: |---------------------------|\n",
      "Train Loss: 5.010724197651283\n",
      "Train Accuracy: 1.905\n",
      "Eval Loss: 4.786122136814579\n",
      "Eval Accuracy: 3.99\n",
      "|---------------------------| Start Epoch 4: |---------------------------|\n",
      "Train Loss: 4.919468417811379\n",
      "Train Accuracy: 2.563\n",
      "Eval Loss: 4.629715615776694\n",
      "Eval Accuracy: 5.46\n",
      "|---------------------------| Start Epoch 5: |---------------------------|\n",
      "Train Loss: 4.787711892155448\n",
      "Train Accuracy: 3.487\n",
      "Eval Loss: 4.484924143287027\n",
      "Eval Accuracy: 6.94\n",
      "|---------------------------| Start Epoch 6: |---------------------------|\n",
      "Train Loss: 4.530812375604992\n",
      "Train Accuracy: 6.061\n",
      "Eval Loss: 4.008611366247675\n",
      "Eval Accuracy: 12.3\n",
      "|---------------------------| Start Epoch 7: |---------------------------|\n",
      "Train Loss: 4.085776653796224\n",
      "Train Accuracy: 11.582\n",
      "Eval Loss: 3.7057516073725028\n",
      "Eval Accuracy: 17.63\n",
      "|---------------------------| Start Epoch 8: |---------------------------|\n",
      "Train Loss: 3.7705499588947453\n",
      "Train Accuracy: 16.442\n",
      "Eval Loss: 3.416867901565163\n",
      "Eval Accuracy: 22.69\n",
      "|---------------------------| Start Epoch 9: |---------------------------|\n",
      "Train Loss: 3.553404693182508\n",
      "Train Accuracy: 20.108\n",
      "Eval Loss: 3.2699485903332945\n",
      "Eval Accuracy: 25.84\n",
      "|---------------------------| Start Epoch 10: |---------------------------|\n",
      "Train Loss: 3.3818498396065015\n",
      "Train Accuracy: 22.73\n",
      "Eval Loss: 3.1553223983497376\n",
      "Eval Accuracy: 27.39\n",
      "|---------------------------| Start Epoch 11: |---------------------------|\n",
      "Train Loss: 3.2412042816098654\n",
      "Train Accuracy: 25.261\n",
      "Eval Loss: 3.093601304254714\n",
      "Eval Accuracy: 29.05\n",
      "|---------------------------| Start Epoch 12: |---------------------------|\n",
      "Train Loss: 3.1218680586482344\n",
      "Train Accuracy: 27.365\n",
      "Eval Loss: 2.9897851959155624\n",
      "Eval Accuracy: 30.61\n",
      "|---------------------------| Start Epoch 13: |---------------------------|\n",
      "Train Loss: 3.0119737887794362\n",
      "Train Accuracy: 29.336\n",
      "Eval Loss: 2.967899235950154\n",
      "Eval Accuracy: 30.87\n",
      "|---------------------------| Start Epoch 14: |---------------------------|\n",
      "Train Loss: 2.9302068742055276\n",
      "Train Accuracy: 30.745\n",
      "Eval Loss: 2.9132650126317503\n",
      "Eval Accuracy: 32.32\n",
      "|---------------------------| Start Epoch 15: |---------------------------|\n",
      "Train Loss: 2.8438761378432877\n",
      "Train Accuracy: 32.239\n",
      "Eval Loss: 2.8884198346715064\n",
      "Eval Accuracy: 33.2\n",
      "|---------------------------| Start Epoch 16: |---------------------------|\n",
      "Train Loss: 2.7638717176665577\n",
      "Train Accuracy: 33.612\n",
      "Eval Loss: 2.8757282685322365\n",
      "Eval Accuracy: 33.29\n",
      "|---------------------------| Start Epoch 17: |---------------------------|\n",
      "Train Loss: 2.6906113285936\n",
      "Train Accuracy: 35.046\n",
      "Eval Loss: 2.861715067723754\n",
      "Eval Accuracy: 33.46\n",
      "|---------------------------| Start Epoch 18: |---------------------------|\n",
      "Train Loss: 2.6253513375384183\n",
      "Train Accuracy: 36.182\n",
      "Eval Loss: 2.844556740135144\n",
      "Eval Accuracy: 34.19\n",
      "|---------------------------| Start Epoch 19: |---------------------------|\n",
      "Train Loss: 2.56276919371946\n",
      "Train Accuracy: 37.253\n",
      "Eval Loss: 2.8212877716987754\n",
      "Eval Accuracy: 34.08\n",
      "|---------------------------| Start Epoch 20: |---------------------------|\n",
      "Train Loss: 2.5023580887953747\n",
      "Train Accuracy: 38.189\n",
      "Eval Loss: 2.8144582289799005\n",
      "Eval Accuracy: 34.13\n",
      "|---------------------------| Start Epoch 21: |---------------------------|\n",
      "Train Loss: 2.4461855475748493\n",
      "Train Accuracy: 39.18\n",
      "Eval Loss: 2.79072164274325\n",
      "Eval Accuracy: 35.42\n",
      "|---------------------------| Start Epoch 22: |---------------------------|\n",
      "Train Loss: 2.394335173904629\n",
      "Train Accuracy: 40.281\n",
      "Eval Loss: 2.851527634699633\n",
      "Eval Accuracy: 33.97\n",
      "|---------------------------| Start Epoch 23: |---------------------------|\n",
      "Train Loss: 2.3523925614524788\n",
      "Train Accuracy: 40.965\n",
      "Eval Loss: 2.864841470293179\n",
      "Eval Accuracy: 33.36\n",
      "|---------------------------| Start Epoch 24: |---------------------------|\n",
      "Train Loss: 2.3047462482903907\n",
      "Train Accuracy: 41.884\n",
      "Eval Loss: 2.807154822501407\n",
      "Eval Accuracy: 34.51\n",
      "|---------------------------| Start Epoch 25: |---------------------------|\n",
      "Train Loss: 2.2593243967167322\n",
      "Train Accuracy: 42.746\n",
      "Eval Loss: 2.805501319800213\n",
      "Eval Accuracy: 34.55\n",
      "|---------------------------| Start Epoch 26: |---------------------------|\n",
      "Train Loss: 2.2207855272201567\n",
      "Train Accuracy: 43.592\n",
      "Eval Loss: 2.801990715561399\n",
      "Eval Accuracy: 34.68\n",
      "|---------------------------| Start Epoch 27: |---------------------------|\n",
      "Train Loss: 2.1872322698167252\n",
      "Train Accuracy: 44.051\n",
      "Eval Loss: 2.8470746559701907\n",
      "Eval Accuracy: 34.49\n",
      "|---------------------------| Start Epoch 28: |---------------------------|\n",
      "Train Loss: 2.1581719439531546\n",
      "Train Accuracy: 44.749\n",
      "Eval Loss: 2.7840312438406003\n",
      "Eval Accuracy: 35.51\n",
      "|---------------------------| Start Epoch 29: |---------------------------|\n",
      "Train Loss: 2.1145597340125573\n",
      "Train Accuracy: 45.628\n",
      "Eval Loss: 2.849294056558305\n",
      "Eval Accuracy: 34.8\n",
      "|---------------------------| Start Epoch 30: |---------------------------|\n",
      "Train Loss: 2.089326157603444\n",
      "Train Accuracy: 45.921\n",
      "Eval Loss: 2.8079074264331987\n",
      "Eval Accuracy: 34.91\n",
      "|---------------------------| Start Epoch 31: |---------------------------|\n",
      "Train Loss: 2.0568058290926983\n",
      "Train Accuracy: 46.725\n",
      "Eval Loss: 2.80891836524769\n",
      "Eval Accuracy: 35.0\n",
      "|---------------------------| Start Epoch 32: |---------------------------|\n",
      "Train Loss: 2.0255169225517022\n",
      "Train Accuracy: 47.303\n",
      "Eval Loss: 2.832451765704307\n",
      "Eval Accuracy: 35.48\n",
      "|---------------------------| Start Epoch 33: |---------------------------|\n",
      "Train Loss: 1.9917106195206986\n",
      "Train Accuracy: 47.982\n",
      "Eval Loss: 2.843949840326977\n",
      "Eval Accuracy: 34.96\n",
      "|---------------------------| Start Epoch 34: |---------------------------|\n",
      "Train Loss: 1.973625848862275\n",
      "Train Accuracy: 48.262\n",
      "Eval Loss: 2.8177614819472003\n",
      "Eval Accuracy: 35.02\n",
      "|---------------------------| Start Epoch 35: |---------------------------|\n",
      "Train Loss: 1.9469756154928617\n",
      "Train Accuracy: 48.961\n",
      "Eval Loss: 2.832014489325748\n",
      "Eval Accuracy: 35.53\n",
      "|---------------------------| Start Epoch 36: |---------------------------|\n",
      "Train Loss: 1.9266024432880942\n",
      "Train Accuracy: 49.317\n",
      "Eval Loss: 2.904072758498465\n",
      "Eval Accuracy: 34.31\n",
      "|---------------------------| Start Epoch 37: |---------------------------|\n",
      "Train Loss: 1.906715323432317\n",
      "Train Accuracy: 49.779\n",
      "Eval Loss: 2.8672369012407435\n",
      "Eval Accuracy: 35.04\n",
      "|---------------------------| Start Epoch 38: |---------------------------|\n",
      "Train Loss: 1.8819065527967818\n",
      "Train Accuracy: 50.319\n",
      "Eval Loss: 2.825311663803781\n",
      "Eval Accuracy: 35.34\n",
      "|---------------------------| Start Epoch 39: |---------------------------|\n",
      "Train Loss: 1.8665697554022702\n",
      "Train Accuracy: 50.591\n",
      "Eval Loss: 2.831348892989432\n",
      "Eval Accuracy: 35.04\n",
      "|---------------------------| Start Epoch 40: |---------------------------|\n",
      "Train Loss: 1.8407213312650597\n",
      "Train Accuracy: 50.784\n",
      "Eval Loss: 2.8590411167995184\n",
      "Eval Accuracy: 35.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "%env WANDB_SILENT=true\n",
    "%env PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "try:\n",
    "    wandb.login()\n",
    "    wandb.init(project=\"del\",\n",
    "               entity=\"hariveliki\")\n",
    "    wandb.define_metric(\"epoch\")\n",
    "    wandb.define_metric(\"loss_train\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"loss_eval\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"train_accuracy\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"eval_accuracy\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"f1_macro\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"precision\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"recall\", step_metric=\"epoch\")\n",
    "\n",
    "    model.to(device)\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print(f\"|---------------------------| Start Epoch {epoch}: |---------------------------|\")\n",
    "        loss_train = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        model.train()\n",
    "        for n, batch in enumerate(train_loader):\n",
    "            imgs = batch[\"image\"]\n",
    "            imgs = imgs.to(device)\n",
    "            labels = batch[\"label\"]\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            labels = labels.long()\n",
    "            logits = model(imgs)\n",
    "            preds = nn.functional.softmax(logits, dim=1)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss_train += loss.item()\n",
    "            predicted = preds.argmax(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        loss_train /= len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        print(f\"Train Loss: {loss_train}\")\n",
    "        print(f\"Train Accuracy: {train_accuracy}\")\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_eval = 0\n",
    "        model.eval()\n",
    "        for n, batch in enumerate(val_loader):\n",
    "            imgs = batch[\"image\"]\n",
    "            imgs = imgs.to(device)\n",
    "            labels = batch[\"label\"]\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            labels = labels.long()\n",
    "            logits = model(imgs)\n",
    "            preds = nn.functional.softmax(logits, dim=1)\n",
    "            loss_eval += criterion(logits, labels).item()\n",
    "            predicted = preds.argmax(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        loss_eval /= len(val_loader)\n",
    "        eval_accuracy = 100 * correct / total\n",
    "        print(f\"Eval Loss: {loss_eval}\")\n",
    "        print(f\"Eval Accuracy: {eval_accuracy}\")\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"loss_train\": loss_train,\n",
    "                \"loss_eval\": loss_eval,\n",
    "                \"train_accuracy\": train_accuracy,\n",
    "                \"eval_accuracy\": eval_accuracy\n",
    "            }\n",
    "        )\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "except Exception as e:\n",
    "    wandb.finish()\n",
    "    raise e\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    wandb.finish()\n",
    "    raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD\n",
    "After finding a working learning rate, we train with the entire data set and only with SGD given the architecture below.  \n",
    "We see an increasing eval loss after few epochs which indicates overfitting.  \n",
    "\n",
    "Architecture:  \n",
    "3x3 conv, 16  \n",
    "pool, 2/  \n",
    "3x3 conv, 32  \n",
    "pool, 2/  \n",
    "\n",
    "fc 500  \n",
    "fc 500  \n",
    "fc 200  \n",
    "\n",
    "<img src=\"plots/3_loss_train_eval.png\" width=\"800\" height=\"500\">\n",
    "<img src=\"plots/3_acc_train_eval.png\" width=\"800\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stride\n",
    "We introduced Stride within the convolutions to reduce the number of parameters and decrease the distance between train and evaluation loss.  \n",
    "The introduction of Stride resulted in better performance than before, but still overfitting between 5 and 10 epochs.  \n",
    "In hindsight, it was the wrong decision to introduce Stride into the convolutions, as there was already a pooling layer performing the same dimensionality reduction task.  \n",
    "It was no longer possible to increase the size of the architecture.  \n",
    "In later architecture decisions, we only used pooling without stride in the convolutions.  \n",
    "\n",
    "Architecture:  \n",
    "3x3 conv, 16  (stride)  \n",
    "pool, 2/  \n",
    "3x3 conv, 32  (stride)  \n",
    "pool, 2/  \n",
    "\n",
    "fc 500  \n",
    "fc 500  \n",
    "fc 200  \n",
    "\n",
    "<img src=\"plots/4_loss_train_eval.png\" width=\"800\" height=\"500\">\n",
    "<img src=\"plots/4_acc_train_eval.png\" width=\"800\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding and Stride\n",
    "Based on the same architecture as before with stride we added padding which pads the input volume with zeros around the border.  \n",
    "Therefore we preserve the spatial dimension from decreasing and keep as much information about the original input volume as possible, so that we can extract those low level features.  \n",
    "We see a better performance than before and overfitting only after 10 epochs.  \n",
    "\n",
    "Architecture:  \n",
    "3x3 conv, 16  (stride, padding)  \n",
    "pool, 2/  \n",
    "3x3 conv, 32  (stride, padding)  \n",
    "pool, 2/  \n",
    "\n",
    "fc 500  \n",
    "fc 500  \n",
    "fc 200 \n",
    "\n",
    "<img src=\"plots/5_loss_train_eval.png\" width=\"800\" height=\"500\">\n",
    "<img src=\"plots/5_acc_train_eval.png\" width=\"800\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Architecture\n",
    "After discussions with the expert, we changed the architecture and used more convolution layers and reduced the number of fully connected layers.  \n",
    "We also removed the stride from the convolution layers this time and kept the padding.  \n",
    "This architecture defined our new baseline.  \n",
    "As we now had many more parameters due to the increase in convolution layers, overfitting occurred again very quickly.  \n",
    "\n",
    "Architecture:  \n",
    "3x3 conv, 16  \n",
    "3x3 conv, 32  \n",
    "pool, 2/  \n",
    "3x3 conv, 64  \n",
    "3x3 conv, 64  \n",
    "pool, 2/  \n",
    "3x3 conv, 128  \n",
    "3x3 conv, 128  \n",
    "pool, 2/  \n",
    "\n",
    "fc 500  \n",
    "fc 200  \n",
    "\n",
    "<img src=\"plots/6_loss_train_eval.png\" width=\"800\" height=\"500\">\n",
    "<img src=\"plots/6_acc_train_eval.png\" width=\"800\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization\n",
    "We tried l2 aka weight decay regularization to reduce overfitting.  \n",
    "But comparing the previous result with the regularization, we don't see much difference.  \n",
    "\n",
    "Architecture:  \n",
    "3x3 conv, 16  \n",
    "3x3 conv, 32  \n",
    "pool, 2/  \n",
    "3x3 conv, 64  \n",
    "3x3 conv, 64  \n",
    "pool, 2/  \n",
    "3x3 conv, 128  \n",
    "3x3 conv, 128  \n",
    "pool, 2/  \n",
    "\n",
    "fc 500  \n",
    "fc 200  \n",
    "\n",
    "<img src=\"plots/7_6_loss_train_eval.png\" width=\"800\" height=\"500\">\n",
    "<img src=\"plots/7_6_acc_train_eval.png\" width=\"800\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "Using another regularization technique called dropout gave us much better results.  \n",
    "We see overfitting only after 20 epochs.  \n",
    "The green curves are with l2 regularization and the orange curves are with dropout.  \n",
    "\n",
    "Architecture:  \n",
    "3x3 conv, 16  \n",
    "3x3 conv, 32  \n",
    "pool, 2/  \n",
    "3x3 conv, 64  \n",
    "3x3 conv, 64  \n",
    "pool, 2/  \n",
    "3x3 conv, 128  \n",
    "3x3 conv, 128  \n",
    "pool, 2/  \n",
    "\n",
    "dropout\n",
    "fc 500  \n",
    "dropout   \n",
    "fc 200  \n",
    "\n",
    "<img src=\"plots/8_7_loss_train_eval.png\" width=\"800\" height=\"500\">\n",
    "<img src=\"plots/8_7_acc_train_eval.png\" width=\"800\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Norm\n",
    "Now we have tried batch norm and investigated how it affects the performance of the model.  \n",
    "It resulted in faster and lower losses and higher accuracy compared to dropout, but was not as good at generalizing to unseen data.  \n",
    "The red curves are with dropout and the orange curves are with batch norm.  \n",
    "\n",
    "Architecture:  \n",
    "3x3 conv, 16  (batchnorm)  \n",
    "3x3 conv, 32  (batchnorm)  \n",
    "pool, 2/  \n",
    "3x3 conv, 64  (batchnorm)  \n",
    "3x3 conv, 64  (batchnorm)  \n",
    "pool, 2/  \n",
    "3x3 conv, 128  (batchnorm)  \n",
    "3x3 conv, 128  (batchnorm)  \n",
    "pool, 2/  \n",
    "\n",
    "fc 500  \n",
    "fc 200  \n",
    "\n",
    "<img src=\"plots/8_loss_train_eval.png\" width=\"800\" height=\"500\">\n",
    "<img src=\"plots/8_acc_train_eval.png\" width=\"800\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dropout and Batch Norm  \n",
    "Drop out and batch norm combined gave us the best result so far.  \n",
    "The loss curves and the small gap between eval and train indicate that we could run more epochs and get even better results.  \n",
    "\n",
    "Architecture:  \n",
    "3x3 conv, 16  (batchnorm)  \n",
    "3x3 conv, 32  (batchnorm)  \n",
    "pool, 2/  \n",
    "3x3 conv, 64  (batchnorm)  \n",
    "3x3 conv, 64  (batchnorm)  \n",
    "pool, 2/  \n",
    "3x3 conv, 128  (batchnorm)  \n",
    "3x3 conv, 128  (batchnorm)  \n",
    "pool, 2/   \n",
    "\n",
    "dropout  \n",
    "fc 500  \n",
    "dropout   \n",
    "fc 200  \n",
    "\n",
    "<img src=\"plots/8_do_bn_loss_train_eval.png\" width=800 height=500>\n",
    "<img src=\"plots/8_do_bn_acc_train_eval.png\" width=800 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimizer\n",
    "Finally using Adam as optimizer lead to a much faster convergence, we achieved better results in less epochs.  \n",
    "We could stop the training after 15 epochs and still get better results than before.  \n",
    "\n",
    "Architecture:  \n",
    "3x3 conv, 16  (batchnorm)  \n",
    "3x3 conv, 32  (batchnorm)  \n",
    "pool, 2/  \n",
    "3x3 conv, 64  (batchnorm)  \n",
    "3x3 conv, 64  (batchnorm)  \n",
    "pool, 2/  \n",
    "3x3 conv, 128  (batchnorm)  \n",
    "3x3 conv, 128  (batchnorm)  \n",
    "pool, 2/   \n",
    "\n",
    "dropout  \n",
    "fc 500  \n",
    "dropout   \n",
    "fc 200  \n",
    "\n",
    "<img src=\"plots/9_8_loss_train_eval.png\" width=\"800\" height=\"500\">\n",
    "<img src=\"plots/9_8_acc_train_eval.png\" width=\"800\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "del",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
