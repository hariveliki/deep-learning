{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Select Task/Dataset\n",
    "I chose Tiny ImageNet, which contains 100000 images of 200 classes (500 for each class) resized to 64x64 color images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from classes import i2d\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/haris.alic/.cache/huggingface/datasets/Maysee___parquet/Maysee--tiny-imagenet-2eb6c3acd8ebc62a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('Maysee/tiny-imagenet', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64>,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset_infos.json\") as file:\n",
    "    dataset_infos = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = dataset_infos[\"Maysee--tiny-imagenet\"][\"features\"][\"label\"][\"names\"]\n",
    "idx2class = {i: class_names[i] for i in range(len(class_names))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Get to know the data\n",
    "The dataset is well balanced and has 500 images for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# class_counts = defaultdict(int)\n",
    "# for instance in dataset:\n",
    "#     label = instance['label']\n",
    "#     class_counts[label] += 1\n",
    "\n",
    "# for label, count in class_counts.items():\n",
    "#     print(f\"Class {label}: {count} instances\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Structure Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1 Determine how (with which metrics) you want to evaluate your model. Also, consider the error in estimating the metrics.\n",
    "We will use accuracy and F1 macro (precision, recall) to evaluate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2 Implement basic functionality to train models and evaluate them against each other. It is recommended to use a suitable MLOps platform (e.g. W&B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tin import TinyImageNetDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded7f86851fd4ce8b391b1e2e1a0b4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preloading train data...:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1075646cd0f84445a004a802b9f60669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preloading val data...:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510ecfe72a77485c98f2bce7499f0c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preloading test data...:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a custom Dataset class because the dataset from load_dataset() is useless\n",
    "train_data = TinyImageNetDataset(root_dir=\"./data/tiny-imagenet-200\", mode=\"train\")\n",
    "val_data = TinyImageNetDataset(root_dir=\"./data/tiny-imagenet-200\", mode=\"val\")\n",
    "test_data = TinyImageNetDataset(root_dir=\"./data/tiny-imagenet-200\", mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data size: 100000\n",
      "val_data size: 10000\n",
      "test_data size: 10000\n"
     ]
    }
   ],
   "source": [
    "# reduce the size of train_data by x\n",
    "train_data = torch.utils.data.Subset(train_data, range(0, len(train_data), 1))\n",
    "print(f\"train_data size: {len(train_data)}\")\n",
    "val_data = torch.utils.data.Subset(val_data, range(0, len(val_data), 1))\n",
    "print(f\"val_data size: {len(val_data)}\")\n",
    "test_data = torch.utils.data.Subset(test_data, range(0, len(test_data), 1))\n",
    "print(f\"test_data size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12500 batches in the training set\n",
      "There are 1250 batches in the validation set\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(f\"There are {len(train_loader)} batches in the training set\")\n",
    "print(f\"There are {len(val_loader)} batches in the validation set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_MLP(nn.Module):\n",
    "    def __init__(self, num_classes=200, weight_init=\"random_uniform\"):\n",
    "        super(CNN_MLP, self).__init__()\n",
    "        # Convolutional Layer that takes an input tensor with 3 channels\n",
    "        # and outputs a tensor with 16 channels\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,\n",
    "                               out_channels=16,\n",
    "                               kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16,\n",
    "                               out_channels=32,\n",
    "                               kernel_size=3)\n",
    "        # Flattens the input tensor into a 1D tensor\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        # Weight Initialization\n",
    "        self.initialize_weights(weight_init)\n",
    "        if weight_init == \"random_uniform\":\n",
    "            assert (self.conv1.weight >= 0).all() and (self.conv1.weight <= 1).all()\n",
    "            assert (self.fc1.weight >= 0).all() and (self.fc1.weight <= 1).all()\n",
    "        elif weight_init == \"random_normal\":\n",
    "            assert torch.isclose(self.conv1.weight.mean(), torch.tensor(0.), atol=1e-2).item()\n",
    "            assert torch.isclose(self.conv1.weight.std(), torch.tensor(1.), atol=1e-2).item()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.permute(0, 3, 1, 2)  # From (batch_size, H, W, C) to (batch_size, C, H, W)\n",
    "        assert x.shape == (batch_size, 3, 64, 64)\n",
    "        \n",
    "        # # TODO understand -> using ReLU (non-saturating activation functions) to alleviate the vanishing gradients problem\n",
    "        x = self.conv1(x)\n",
    "        assert x.shape == (batch_size, 16, 62, 62)\n",
    "        x = nn.ReLU()(x)\n",
    "        # Applies max-pooling to reduce the spatial dimensions of the tensor\n",
    "        x = nn.MaxPool2d(kernel_size=2)(x)\n",
    "        assert x.shape == (batch_size, 16, 31, 31)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        assert x.shape == (batch_size, 32, 29, 29)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = nn.MaxPool2d(kernel_size=2)(x)\n",
    "        assert x.shape == (batch_size, 32, 14, 14)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = nn.ReLU()(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def initialize_weights(self, kind):\n",
    "        if kind == \"random_uniform\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    nn.init.uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.uniform_(m.bias)\n",
    "                elif isinstance(m, nn.Linear):\n",
    "                    nn.init.uniform_(m.weight)\n",
    "                    nn.init.uniform_(m.bias)\n",
    "        elif kind == \"random_normal\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    nn.init.normal_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.normal_(m.bias)\n",
    "                elif isinstance(m, nn.Linear):\n",
    "                    nn.init.normal_(m.weight)\n",
    "                    nn.init.normal_(m.bias)\n",
    "        elif kind == \"xavier\":\n",
    "            pass\n",
    "        elif kind == \"he\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Invalid weight initialization kind!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_MLP(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=6272, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=200, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CNN_MLP(num_classes=200, weight_init=\"random_uniform\")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=true\n",
      "env: PYTORCH_ENABLE_MPS_FALLBACK=1\n"
     ]
    }
   ],
   "source": [
    "# %env WANDB_LOG_MODEL=\"end\"\n",
    "%env WANDB_SILENT=true\n",
    "%env PYTORCH_ENABLE_MPS_FALLBACK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|---------------------------| Start Epoch 1: |---------------------------|\n",
      "Train Loss: 5.434091567993164\n",
      "Eval Loss: 0.533295962600708\n",
      "Accuracy: 0.5\n",
      "F1 Macro: 4.975124378109453e-05\n",
      "Precision: 0.005\n",
      "Recall: 2.5e-05\n",
      "|---------------------------| Start Epoch 2: |---------------------------|\n",
      "Train Loss: 5.341231346130371\n",
      "Eval Loss: 0.5332529539108276\n",
      "Accuracy: 0.5\n",
      "F1 Macro: 4.975124378109453e-05\n",
      "Precision: 0.005\n",
      "Recall: 2.5e-05\n",
      "|---------------------------| Start Epoch 3: |---------------------------|\n",
      "Train Loss: 5.221280574798584\n",
      "Eval Loss: 0.5332105222702026\n",
      "Accuracy: 0.5\n",
      "F1 Macro: 4.975124378109453e-05\n",
      "Precision: 0.005\n",
      "Recall: 2.5e-05\n",
      "|---------------------------| Start Epoch 4: |---------------------------|\n",
      "Train Loss: 5.344844341278076\n",
      "Eval Loss: 0.5331686963272094\n",
      "Accuracy: 0.5\n",
      "F1 Macro: 4.975124378109453e-05\n",
      "Precision: 0.005\n",
      "Recall: 2.5e-05\n",
      "|---------------------------| Start Epoch 5: |---------------------------|\n",
      "Train Loss: 5.310181617736816\n",
      "Eval Loss: 0.5331276106262207\n",
      "Accuracy: 0.5\n",
      "F1 Macro: 4.975124378109453e-05\n",
      "Precision: 0.005\n",
      "Recall: 2.5e-05\n",
      "|---------------------------| Start Epoch 6: |---------------------------|\n",
      "Train Loss: 5.366397380828857\n",
      "Eval Loss: 0.5330871371459961\n",
      "Accuracy: 0.5\n",
      "F1 Macro: 4.975124378109453e-05\n",
      "Precision: 0.005\n",
      "Recall: 2.5e-05\n",
      "|---------------------------| Start Epoch 7: |---------------------------|\n",
      "Train Loss: 5.2512407302856445\n",
      "Eval Loss: 0.5330471625900268\n",
      "Accuracy: 0.5\n",
      "F1 Macro: 4.975124378109453e-05\n",
      "Precision: 0.005\n",
      "Recall: 2.5e-05\n",
      "|---------------------------| Start Epoch 8: |---------------------------|\n",
      "Train Loss: 5.407412528991699\n",
      "Eval Loss: 0.533007592048645\n",
      "Accuracy: 0.5\n",
      "F1 Macro: 4.975124378109453e-05\n",
      "Precision: 0.005\n",
      "Recall: 2.5e-05\n",
      "|---------------------------| Start Epoch 9: |---------------------------|\n",
      "Train Loss: 5.246845245361328\n",
      "Eval Loss: 0.5329685339736938\n",
      "Accuracy: 0.5\n",
      "F1 Macro: 4.975124378109453e-05\n",
      "Precision: 0.005\n",
      "Recall: 2.5e-05\n",
      "|---------------------------| Start Epoch 10: |---------------------------|\n",
      "Train Loss: 5.456048011779785\n",
      "Eval Loss: 0.5329300969314575\n",
      "Accuracy: 0.5\n",
      "F1 Macro: 4.975124378109453e-05\n",
      "Precision: 0.005\n",
      "Recall: 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "wandb.login()\n",
    "wandb.init(project=\"del\",\n",
    "           entity=\"hariveliki\")\n",
    "wandb.define_metric(\"epoch\")\n",
    "wandb.define_metric(\"loss_train\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"loss_eval\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"accuracy\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"f1_macro\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"precision\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"recall\", step_metric=\"epoch\")\n",
    "\n",
    "# conv1_grads = []\n",
    "# def save_grad(module, grad_input, grad_output):\n",
    "#     conv1_grads.append(grad_output[0])\n",
    "# hook_handle = model.conv1.register_full_backward_hook(save_grad)\n",
    "\n",
    "model.to(device)\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"|---------------------------| Start Epoch {epoch + 1}: |---------------------------|\")\n",
    "    model.train()\n",
    "    for n, batch in enumerate(train_loader):\n",
    "        imgs = batch[\"image\"]\n",
    "        imgs = imgs.to(device)\n",
    "        labels = batch[\"label\"]\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        labels = labels.long()\n",
    "        logits = model(imgs)\n",
    "        loss_train = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Train Loss: {loss_train}\")\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_eval = 0\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    model.eval()\n",
    "    for n, batch in enumerate(val_loader):\n",
    "        imgs = batch[\"image\"]\n",
    "        imgs = imgs.to(device)\n",
    "        labels = batch[\"label\"]\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        labels = labels.long()\n",
    "        logits = model(imgs)\n",
    "        preds = nn.functional.softmax(logits, dim=1)\n",
    "        loss_eval += criterion(logits, labels).item()\n",
    "        correct += (preds.argmax(1) == labels).sum().item()\n",
    "        all_labels.extend(preds.argmax(1).cpu().detach().numpy().tolist())\n",
    "        all_outputs.extend(labels.cpu().detach().numpy().tolist())\n",
    "\n",
    "    loss_eval /= len(train_loader)\n",
    "    correct /= len(val_data)\n",
    "    accuracy = 100 * correct\n",
    "    f1_macro = f1_score(all_labels, all_outputs, average='macro')\n",
    "    precision = precision_score(all_labels, all_outputs, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_outputs, average='macro', zero_division=0)\n",
    "    print(f\"Eval Loss: {loss_eval}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Macro: {f1_macro}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    wandb.log(\n",
    "        {\"loss_train\": loss_train,\n",
    "         \"epoch\": epoch,\n",
    "         \"loss_eval\": loss_eval,\n",
    "         \"accuracy\": accuracy,\n",
    "         \"f1_macro\": f1_macro,\n",
    "         \"precision\": precision,\n",
    "         \"recall\": recall}\n",
    "    )\n",
    "\n",
    "# hook_handle.remove()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_conv_layer_gradients(batch_idx, conv_grads):\n",
    "    \"\"\"\n",
    "    Plots the gradients of a convolutional layer for a given batch index.\n",
    "\n",
    "    Args:\n",
    "    - batch_idx (int): The index of the batch to plot the gradients for.\n",
    "    - conv_grads (list): A list of the gradients of the convolutional layer.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Get the gradient for that batch\n",
    "    batch_grad = conv_grads[batch_idx]\n",
    "\n",
    "    # Convert the PyTorch tensor to a NumPy array\n",
    "    batch_grad_np = batch_grad.cpu().numpy()\n",
    "\n",
    "    # Create a plot\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # Assuming the weight tensor is of shape (out_channels, in_channels, kernel_size, kernel_size)\n",
    "    out_channels = batch_grad_np.shape[0]\n",
    "    for i in range(out_channels):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(batch_grad_np[i, 0], cmap='viridis')\n",
    "        plt.title(f'Output channel {i}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# plot_conv_layer_gradients(0, conv1_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| None                                                                                                                                                                                                                                                                                                                                                                              | Hyperparameter                 | SGD  | REG   | BN    | ADAM  | Weight Init    | Theorie Expectation | Actual | Conclusion |\n",
    "| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------ | ---- | ----- | ----- | ----- | -------------- | ------------------- | ------ | ---------- |\n",
    "| ![[Pasted image 20230926162413.png]] | Epochs: 5 <br/> LR: 1e-5 <br/> | True | False | False | False | Random Uniform | None                | None   | None       |\n",
    "|                                                                                                                                                                                                                                                                                                                                                                                   |                                |      |       |       |       |                |                     |        |            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "del",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
